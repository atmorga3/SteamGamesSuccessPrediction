{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda')\n",
        "device = torch.device('cpu')\n",
        "\n"
      ],
      "metadata": {
        "id": "zfHztKoZqADR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot = pd.read_csv('OneHotVectors.csv').fillna(0.0)"
      ],
      "metadata": {
        "id": "4RxtA7TSqa2c"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class steamGamesDataset(Dataset):\n",
        "    def __init__(self, lhs_dataframe, rhs_dataframe, device='cpu'):\n",
        "        self.lhs_df = lhs_dataframe\n",
        "        self.rhs_df = rhs_dataframe\n",
        "        # self.dataset = pd.concat([lhs_dataframe, rhs_dataframe], axis=1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lhs_df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        inp_features = torch.tensor(self.lhs_df.iloc[index]).to(torch.float32)\n",
        "        value = torch.tensor(self.rhs_df.iloc[index]).to(torch.float32)\n",
        "        return inp_features, value"
      ],
      "metadata": {
        "id": "E8LmCBdyqlNk"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "eR6fxZH1p4QO"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self,input_size, squeeze_size, device='cpu'):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, 640),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(640, 320),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(320, 160))\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(160, 320),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(320, 640),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(640, 740),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(740, input_size),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        dec = self.decoder(enc)\n",
        "        return dec\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoEncoder_dataset = steamGamesDataset(onehot, onehot)\n",
        "autoEncoder_dataloader = data.DataLoader(autoEncoder_dataset, batch_size=12024)\n"
      ],
      "metadata": {
        "id": "fQaEGC18qi8f"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoEncoder(834, 1, device=device).to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5, weight_decay = 1e-8)\n"
      ],
      "metadata": {
        "id": "VCWlxTElqopn"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "outputs = []\n",
        "losses = []\n",
        "i = 0\n",
        "for epoch in range(epochs):\n",
        "  print(f'Epoch: {epoch}', end='\\r')\n",
        "  for (image, output) in autoEncoder_dataloader:\n",
        "    i += 1\n",
        "    if i % 10: print(f'itteration: {i}', end='\\r')\n",
        "    image = image.to(device)\n",
        "    output = output.to(device)\n",
        "    # Reshaping the image to (-1, 784)\n",
        "    #   image = image.reshape(-1, 28*28)\n",
        "\n",
        "    # Output of Autoencoder\n",
        "    reconstructed = model(image)\n",
        "\n",
        "    # Calculating the loss function\n",
        "    loss = loss_function(reconstructed, image)\n",
        "\n",
        "    # The gradients are set to zero,\n",
        "    # the gradient is computed and stored.\n",
        "    # .step() performs parameter update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\t\t# Storing the losses in a list for plotting\n",
        "    losses.append(loss.detach())\n",
        "  # outputs.append((epochs, image, reconstructed))\n",
        "\n",
        "# # Defining the Plot Style\n",
        "# plt.style.use('fivethirtyeight')\n",
        "# plt.xlabel('Iterations')\n",
        "# plt.ylabel('Loss')\n",
        "\n",
        "# # Plotting the last 100 values\n",
        "# plt.plot(losses[-100:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gr21ESIIq7ev",
        "outputId": "b56d8f9c-387c-4b09-ed5c-d4d63800cb6c"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('fivethirtyeight')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Plotting the last 100 values\n",
        "plt.plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "Rjn26gc9Muoo",
        "outputId": "6d4151df-af25-4b79-a9d8-00bbb2ab064c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-bcef8c3860b0>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Plotting the last 100 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVisibleDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0;31m# NumPy 1.19 will warn on ragged input, and we can't actually use it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m             len(x.shape) < 1):\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1062\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1063\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.encoder.state_dict(), '../encoderparams1.pt')"
      ],
      "metadata": {
        "id": "p2pWWDi9AeX_"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class encoder(nn.Module):\n",
        "    def __init__(self,input_size, squeeze_size, device='cpu'):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self._encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, 640),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(640, 320),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(320, 160))\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc = self._encoder(x)\n",
        "        # dec = self.decoder(enc)\n",
        "        return enc\n"
      ],
      "metadata": {
        "id": "_N1v1-cWBBnk"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = encoder(834, 1).to(device)\n",
        "encoder_model._encoder.load_state_dict(torch.load('/encoderparams.pt'))\n",
        "encoder_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64itrbADBNNL",
        "outputId": "8e667cc9-074b-454f-f4f4-95dd6bb06ed1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "encoder(\n",
              "  (_encoder): Sequential(\n",
              "    (0): Linear(in_features=834, out_features=640, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=640, out_features=320, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=320, out_features=160, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = pd.DataFrame()\n",
        "encoded_list = []\n",
        "for inp, _ in autoEncoder_dataloader:\n",
        "    # encoder_model.eval()\n",
        "    inp = inp.to(device)\n",
        "    output = encoder_model.forward(inp)\n",
        "    print(output)\n",
        "    encoded_list.append(pd.DataFrame(output.detach().cpu().numpy()))\n",
        "    # encoded = pd.concat([encoded, pd.DataFrame(output.detach().cpu().numpy())])\n",
        "\n",
        "encoded = pd.concat(encoded_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvTZ4bQtDBct",
        "outputId": "e968c7b6-b4cc-407c-8c77-3ad1c404b30e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 9.4751e-03, -2.3631e-03, -3.4093e-03,  ...,  1.5877e-02,\n",
            "          6.4735e-03,  1.2219e-02],\n",
            "        [ 1.4885e-02,  3.5324e-02,  1.6593e-02,  ..., -1.8814e-02,\n",
            "          3.5429e-02, -1.2247e-02],\n",
            "        [ 9.5165e-03,  5.0621e-03, -5.0812e-04,  ...,  2.4769e-02,\n",
            "          9.6011e-03,  1.7661e-02],\n",
            "        ...,\n",
            "        [ 8.8675e-01,  7.4655e+01, -9.7546e+00,  ...,  1.1955e+01,\n",
            "          5.3752e+01, -3.2640e+01],\n",
            "        [ 8.7252e-01,  7.4630e+01, -9.7438e+00,  ...,  1.1957e+01,\n",
            "          5.3747e+01, -3.2615e+01],\n",
            "        [ 8.8809e-01,  7.4633e+01, -9.7256e+00,  ...,  1.1973e+01,\n",
            "          5.3761e+01, -3.2616e+01]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([[  0.8720,  74.6358,  -9.7593,  ...,  11.9852,  53.7524, -32.6177],\n",
            "        [  0.8910,  74.6493,  -9.7561,  ...,  11.9567,  53.7622, -32.6433],\n",
            "        [  0.8382,  74.6637,  -9.7665,  ...,  11.9652,  53.7716, -32.6473],\n",
            "        ...,\n",
            "        [  1.7524, 149.2886, -19.5012,  ...,  23.8833, 107.4861, -65.2574],\n",
            "        [  1.7675, 149.2699, -19.4942,  ...,  23.8769, 107.4821, -65.2591],\n",
            "        [  1.7469, 149.2882, -19.5109,  ...,  23.8939, 107.5009, -65.2550]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([[  1.7521, 149.2923, -19.5083,  ...,  23.8875, 107.5022, -65.2543],\n",
            "        [  1.7538, 149.3057, -19.5032,  ...,  23.8884, 107.5107, -65.2668],\n",
            "        [  1.7631, 149.2980, -19.4655,  ...,  23.8780, 107.4936, -65.2725],\n",
            "        ...,\n",
            "        [  2.6407, 223.9147, -29.2294,  ...,  35.8231, 161.2222, -97.8930],\n",
            "        [  2.6381, 223.9410, -29.2541,  ...,  35.8241, 161.2295, -97.9075],\n",
            "        [  2.6489, 223.9393, -29.2559,  ...,  35.8122, 161.2370, -97.8822]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([[   2.6326,  223.9291,  -29.2407,  ...,   35.8187,  161.2260,\n",
            "          -97.9017],\n",
            "        [   2.6496,  223.9369,  -29.2414,  ...,   35.8234,  161.2350,\n",
            "          -97.8875],\n",
            "        [   2.6030,  223.9486,  -29.2500,  ...,   35.8256,  161.1871,\n",
            "          -97.9624],\n",
            "        ...,\n",
            "        [   3.5390,  298.5579,  -38.9945,  ...,   47.7446,  214.9646,\n",
            "         -130.5363],\n",
            "        [   3.5236,  298.5669,  -38.9771,  ...,   47.7283,  214.9534,\n",
            "         -130.5596],\n",
            "        [   3.5220,  298.5777,  -39.0049,  ...,   47.7530,  214.9839,\n",
            "         -130.5473]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([[   3.5345,  298.5930,  -38.9864,  ...,   47.7451,  214.9992,\n",
            "         -130.5408],\n",
            "        [   3.4947,  298.5937,  -38.9978,  ...,   47.7743,  215.0049,\n",
            "         -130.5441],\n",
            "        [   3.5358,  298.5854,  -38.9832,  ...,   47.7063,  214.9675,\n",
            "         -130.5647],\n",
            "        ...,\n",
            "        [   4.4203,  373.2188,  -48.7429,  ...,   59.6465,  268.7253,\n",
            "         -163.1849],\n",
            "        [   4.4065,  373.2277,  -48.7253,  ...,   59.6982,  268.7324,\n",
            "         -163.1801],\n",
            "        [   4.4332,  373.2250,  -48.7568,  ...,   59.6795,  268.7119,\n",
            "         -163.1925]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([[   4.3959,  373.2101,  -48.7283,  ...,   59.6721,  268.6872,\n",
            "         -163.1793],\n",
            "        [   4.4060,  373.2433,  -48.7407,  ...,   59.6684,  268.7334,\n",
            "         -163.1704],\n",
            "        [   4.4013,  373.2351,  -48.7649,  ...,   59.6899,  268.7488,\n",
            "         -163.1819],\n",
            "        ...,\n",
            "        [   5.2457,  445.2113,  -58.1077,  ...,   71.1622,  320.5587,\n",
            "         -194.6528],\n",
            "        [   5.2550,  445.1927,  -58.1362,  ...,   71.1699,  320.5482,\n",
            "         -194.6392],\n",
            "        [   5.2420,  445.1992,  -58.1141,  ...,   71.1650,  320.5251,\n",
            "         -194.6806]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTPkbqsDERKc",
        "outputId": "bfcbbe08-5d14-468c-ab46-9434187f2121"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            0           1          2           3           4          5    \\\n",
            "0      0.009475   -0.002363  -0.003409    0.041208    0.026933  -0.047063   \n",
            "1      0.014885    0.035324   0.016593    0.032428    0.043128  -0.061682   \n",
            "2      0.009517    0.005062  -0.000508    0.028606    0.021240  -0.044766   \n",
            "3      0.011100    0.037920  -0.002507   -0.000321    0.057784  -0.049342   \n",
            "4      0.013272    0.032456  -0.001102    0.026311    0.022700  -0.064703   \n",
            "...         ...         ...        ...         ...         ...        ...   \n",
            "11591  5.265992  445.184296 -58.145466 -111.409401  355.649536 -75.058167   \n",
            "11592  5.208364  445.205536 -58.152615 -111.442291  355.712585 -75.028030   \n",
            "11593  5.245724  445.211334 -58.107674 -111.414742  355.687378 -75.073235   \n",
            "11594  5.255037  445.192749 -58.136223 -111.423149  355.689880 -75.051300   \n",
            "11595  5.242000  445.199188 -58.114120 -111.426666  355.692169 -75.061447   \n",
            "\n",
            "             6          7          8          9    ...         150  \\\n",
            "0       0.057828   0.016771   0.031985  -0.033328  ...   -0.002616   \n",
            "1       0.060767  -0.003148   0.032233  -0.005404  ...   -0.038796   \n",
            "2       0.066887   0.014834   0.049541  -0.046537  ...    0.006002   \n",
            "3       0.060157   0.002373   0.021933  -0.031367  ...   -0.010206   \n",
            "4       0.071556   0.007931   0.052506  -0.046703  ...   -0.005168   \n",
            "...          ...        ...        ...        ...  ...         ...   \n",
            "11591  79.939491  79.883797 -89.401291  52.104507  ... -290.524353   \n",
            "11592  79.963936  79.895676 -89.413300  52.109840  ... -290.525238   \n",
            "11593  79.950638  79.883064 -89.408020  52.092564  ... -290.532135   \n",
            "11594  79.925278  79.872818 -89.424149  52.125706  ... -290.523560   \n",
            "11595  79.923721  79.876686 -89.401886  52.126675  ... -290.533325   \n",
            "\n",
            "              151        152        153         154         155         156  \\\n",
            "0       -0.025431  -0.042862   0.029937   -0.014235   -0.052418   -0.015257   \n",
            "1       -0.023422  -0.032191  -0.013157   -0.065830   -0.038708    0.013014   \n",
            "2       -0.015387  -0.051268   0.024393   -0.027450   -0.064483   -0.019545   \n",
            "3       -0.021380  -0.025079   0.018858   -0.033379   -0.048360    0.014598   \n",
            "4        0.002943  -0.050535   0.027205   -0.057132   -0.068252   -0.000940   \n",
            "...           ...        ...        ...         ...         ...         ...   \n",
            "11591  277.852600 -47.383278  49.245678 -458.250946 -394.943756  218.277374   \n",
            "11592  277.842255 -47.384686  49.252453 -458.249756 -394.905670  218.351700   \n",
            "11593  277.864929 -47.362385  49.252602 -458.285583 -394.943420  218.312012   \n",
            "11594  277.859009 -47.378525  49.247692 -458.255219 -394.940338  218.294266   \n",
            "11595  277.855103 -47.375599  49.266968 -458.264099 -394.942535  218.328842   \n",
            "\n",
            "             157         158         159  \n",
            "0       0.015877    0.006473    0.012219  \n",
            "1      -0.018814    0.035429   -0.012247  \n",
            "2       0.024769    0.009601    0.017661  \n",
            "3       0.041480    0.040169   -0.033106  \n",
            "4       0.022073    0.017007    0.007547  \n",
            "...          ...         ...         ...  \n",
            "11591  71.168510  320.526215 -194.634674  \n",
            "11592  71.151123  320.534912 -194.670929  \n",
            "11593  71.162178  320.558685 -194.652817  \n",
            "11594  71.169930  320.548248 -194.639175  \n",
            "11595  71.164993  320.525085 -194.680588  \n",
            "\n",
            "[71716 rows x 160 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded.to_csv('/encoded_oh.csv')"
      ],
      "metadata": {
        "id": "Fcw0Nw_NQIC8"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self,input_size, device='cpu'):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.hidden = nn.Sequential(\n",
        "            nn.Linear(input_size, input_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_size, input_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_size, input_size),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(input_size, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.hidden(x)"
      ],
      "metadata": {
        "id": "3zZ0SQhGQa_G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lhs_df = pd.read_csv('/lhsDataset.csv')\n",
        "\n",
        "onehot_enc = pd.read_csv('/encoded_oh.csv')\n",
        "\n",
        "\n",
        "rhs_df = pd.read_csv('/rhs.csv')\n",
        "rhs_df = rhs_df['Median playtime forever'].to_frame()\n",
        "\n",
        "lhs_df = pd.concat([lhs_df, onehot_enc], axis=1)"
      ],
      "metadata": {
        "id": "V_I9690mSEJg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lhs_df = (lhs_df.astype('Float64') - lhs_df.astype('Float64').min()) / (lhs_df.astype('Float64').max() - lhs_df.astype('Float64').min())\n"
      ],
      "metadata": {
        "id": "hrA3pB7JdYWk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lhs_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M81Q91iuSlAk",
        "outputId": "32b2cf30-7d9a-4ba4-d883-b325b78b3768"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(71716, 197)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NN(197, device=device).to(device)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 1e-8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "O6JjkXXdR9g6",
        "outputId": "a63df178-2eb5-48bd-d936-b53864d0dbfd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-72a673d62667>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m197\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1149\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdataset = steamGamesDataset(lhs_df, rhs_df)\n",
        "gdataloader = data.DataLoader(gdataset, batch_size=1024)"
      ],
      "metadata": {
        "id": "kMNS_pWESzx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "outputs = []\n",
        "losses = []\n",
        "i = 0\n",
        "for epoch in range(epochs):\n",
        "  print(f'Epoch: {epoch}', end='\\r')\n",
        "  for (image, output) in gdataloader:\n",
        "    i += 1\n",
        "    if i % 10: print(f'itteration: {i}', end='\\r')\n",
        "    image = image.to(device)\n",
        "    output = output.to(device)\n",
        "    # Reshaping the image to (-1, 784)\n",
        "    #   image = image.reshape(-1, 28*28)\n",
        "\n",
        "    # Output of Autoencoder\n",
        "    reconstructed = model.forward(image)\n",
        "\n",
        "    # Calculating the loss function\n",
        "    loss = loss_function(reconstructed, image)\n",
        "\n",
        "    # The gradients are set to zero,\n",
        "    # the gradient is computed and stored.\n",
        "    # .step() performs parameter update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\t\t# Storing the losses in a list for plotting\n",
        "    losses.append(loss.detach().cpu())\n",
        "    # print(loss._item)\n",
        "  # outputs.append((epochs, image, reconstructed))\n",
        "\n",
        "# Defining the Plot Style\n",
        "plt.style.use('fivethirtyeight')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Plotting the last 100 values\n",
        "plt.plot(losses)"
      ],
      "metadata": {
        "id": "Hq-bZ7EyRcia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, criterion, tuple_model_output=False, n_to_show=1):\n",
        "    test_loss = 0.0\n",
        "    show_predictions = []\n",
        "    show_gt = []\n",
        "    i = 0\n",
        "    for input, label in gdataloader:\n",
        "        input = input.to(torch.float64).to(device)\n",
        "        label = label.to(torch.float64).to(device)\n",
        "\n",
        "        # input = nn.functional.normalize(input)\n",
        "        # label = nn.functional.normalize(label)\n",
        "\n",
        "        print(input)\n",
        "\n",
        "        prediction = model.forward(input)\n",
        "\n",
        "        if i < n_to_show:\n",
        "            show_predictions.append(input)\n",
        "            show_gt.append(label.detach().cpu().numpy())\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        loss = criterion(prediction, label)\n",
        "        test_loss += loss\n",
        "    test_loss = test_loss / dataset_sizes['test']\n",
        "\n",
        "    # line_types = [':', '--', '-.']\n",
        "    # for idx, input in enumerate(show_predictions):\n",
        "    #     if tuple_model_output:\n",
        "    #         pred, hn = model(input)\n",
        "    #         print(f'Hidden {idx}: {hn}')\n",
        "    #     else:\n",
        "    #         pred = model(input)\n",
        "    #     print(f'Input {idx}: {input}')\n",
        "    #     print(f'Prediction {idx}: {pred}')\n",
        "\n",
        "    #     plt.plot(pred.detach().cpu().numpy(), ':', label='prediction_'+str(idx))\n",
        "    # for idx, gt in enumerate(show_gt):\n",
        "    #     plt.plot(1,  gt, label='GT_'+str(idx))\n",
        "    # plt.legend()\n",
        "    print(f'Test Loss: {test_loss}')\n"
      ],
      "metadata": {
        "id": "ArGm2GcJTZh8"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_model(model=NN, criterion=loss_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "LSrHAD8DTujj",
        "outputId": "73154d66-d135-4ba6-a215-cf0e1ba842f8"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.5877e-02,\n",
            "          6.4735e-03,  1.2219e-02],\n",
            "        [ 1.0000e+00,  1.0000e+00,  0.0000e+00,  ..., -1.8814e-02,\n",
            "          3.5429e-02, -1.2247e-02],\n",
            "        [ 2.0000e+00,  2.0000e+00,  0.0000e+00,  ...,  2.4769e-02,\n",
            "          9.6011e-03,  1.7661e-02],\n",
            "        ...,\n",
            "        [ 1.0210e+03,  1.0210e+03,  0.0000e+00,  ...,  1.0550e+00,\n",
            "          4.5436e+00, -2.7851e+00],\n",
            "        [ 1.0220e+03,  1.0220e+03,  0.0000e+00,  ...,  1.0343e+00,\n",
            "          4.5821e+00, -2.7850e+00],\n",
            "        [ 1.0230e+03,  1.0230e+03,  0.0000e+00,  ...,  1.0459e+00,\n",
            "          4.5766e+00, -2.7618e+00]], device='cuda:0', dtype=torch.float64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "NN.forward() missing 1 required positional argument: 'x'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-e6b7d6db6b8e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-122-b0040270eb52>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model, criterion, tuple_model_output, n_to_show)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_to_show\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: NN.forward() missing 1 required positional argument: 'x'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for (image, output) in gdataloader:\n",
        "    if i > 10000:\n",
        "        break\n",
        "    i += 1\n",
        "    if i % 10: print(f'itteration: {i}', end='\\r')\n",
        "    image = image.to(device)\n",
        "    output = output.to(device)\n",
        "    # Reshaping the image to (-1, 784)\n",
        "    #   image = image.reshape(-1, 28*28)\n",
        "\n",
        "    # Output of Autoencoder\n",
        "    reconstructed = model.forward(image)\n",
        "\n",
        "    # Calculating the loss function\n",
        "    loss = loss_function(reconstructed, image)\n",
        "\n",
        "\t\t# Storing the losses in a list for plotting\n",
        "    losses.append(loss.detach().cpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqsSEs__XqiQ",
        "outputId": "22362704-92e7-48d8-d2b4-be9eb34a650a"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "itteration: 214\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1024, 197])) that is different to the input size (torch.Size([1024, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([36, 197])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(losses)/len(losses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8P4Lm-kX0Zg",
        "outputId": "9cc810ef-5657-4efd-83de-602a7da4b20c"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(32635266.)\n"
          ]
        }
      ]
    }
  ]
}